{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683ffcc4",
   "metadata": {},
   "source": [
    "# Fine-tuning a pre-trained Transformer model on a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56eb9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d24c73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0ab0be2065476e96b9c76d006cbd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e24e8190aa4823b188a4061d1699c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5399570902a449ea974e31ae8a48609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69b6afeb40d464daa4d119c807e330e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b595606aa4d4994abee9f8dd78da2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b43333",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"I love this movie\",\n",
    "    \"This film was terrible\",\n",
    "    \"What a fantastic experience\",\n",
    "    \"I hate this so much\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d99349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3821683529.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "encoding= tokenizer(\n",
    "    text, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=32, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    encoding['input_ids'], \n",
    "    encoding['attention_mask'], \n",
    "    labels\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91f8e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Full Bert\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last 2 layers of BERT\n",
    "for layer in model.bert.encoder.layer[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr = 2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b51127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "loss:  0.697765588760376\n",
      "Epoch 2\n",
      "loss:  0.6590269804000854\n",
      "Epoch 3\n",
      "loss:  0.6246843338012695\n",
      "Epoch 4\n",
      "loss:  0.594849169254303\n",
      "Epoch 5\n",
      "loss:  0.5677815079689026\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "\n",
    "        loss = output.loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d26da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction class: [1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "model.eval()\n",
    "\n",
    "test_text = [\n",
    "    \"I really enjoyed this movie\",\n",
    "    \"Terrible film this was\"\n",
    "]\n",
    "\n",
    "text_encoding = tokenizer(\n",
    "    test_text,\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    max_length = 32,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        input_ids = text_encoding['input_ids'],\n",
    "        attention_mask = text_encoding['attention_mask']\n",
    "    )\n",
    "logits = output.logits\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Prediction class:\", prediction.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf6f4de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.11.attention.self.query.weight True\n",
      "bert.encoder.layer.11.attention.self.query.bias True\n",
      "bert.encoder.layer.11.attention.self.key.weight True\n",
      "bert.encoder.layer.11.attention.self.key.bias True\n",
      "bert.encoder.layer.11.attention.self.value.weight True\n",
      "bert.encoder.layer.11.attention.self.value.bias True\n",
      "bert.encoder.layer.11.attention.output.dense.weight True\n",
      "bert.encoder.layer.11.attention.output.dense.bias True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.intermediate.dense.weight True\n",
      "bert.encoder.layer.11.intermediate.dense.bias True\n",
      "bert.encoder.layer.11.output.dense.weight True\n",
      "bert.encoder.layer.11.output.dense.bias True\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "# checking weights were trained or not in different layers\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'encoder.layer.11' in name or \"classifier\" in name:\n",
    "        print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3f640",
   "metadata": {},
   "source": [
    "(even 2 BERT layers â‰ˆ 15M)\n",
    "With extremely small datasets, linear probing often outperforms fine-tuning because frozen pretrained representations are already strong, while fine-tuning introduces overfitting and representation drift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
