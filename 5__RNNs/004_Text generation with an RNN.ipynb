{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183d9a8b-7028-4531-bcc3-2e5e69205f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b0efdc-7db2-4e1f-9fa0-70ae75efda2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2124bd0b-37c2-4bf6-8dea-3304514b3a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dedc409-516e-4478-aa49-93e384b39c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd2c25-fb9a-4c51-b6f3-72f883ab275f",
   "metadata": {},
   "source": [
    "## Process the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e3e91-8a75-4d4e-84c6-4069cabaec6b",
   "metadata": {},
   "source": [
    "### Vectorize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fed09f55-834d-494b-8866-ded5fdda101a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915c0da-a328-4be7-9883-bd58ba4ddbfb",
   "metadata": {},
   "source": [
    "#### StringLookup Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55b334fc-2062-4627-9e67-bf0f58258ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "230f4623-5900-4a6e-98e5-1a0725bccbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[3, 0, 4, 5, 6, 0, 7], [19, 0, 0]]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59bc2679-31d2-42c9-bca3-499bf1da840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05a77cc7-03e6-4949-9812-138c3151bf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'[UNK]', b'c', b'd', b'e', b'[UNK]', b'g'],\n",
       " [b'x', b'[UNK]', b'[UNK]']]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "924b1ebe-0366-4fce-873e-22d88cfbb2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'a[UNK]cde[UNK]g', b'x[UNK][UNK]'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4922f6c-be36-4b2b-a613-aee91528554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5346b26e-483a-4e5c-a6a4-b7e36ccb3976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([ 0,  9, 15, ...,  7,  2,  0], shape=(1115394,))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "011520d1-87ed-48ca-9a6c-9b92943b2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22f7e20b-fd8c-4911-ad1a-0c9da52bb2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK]\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "[UNK]\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48c1e10d-36aa-46dd-8e88-258dc10de653",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d9b5c5e-f233-4c43-9fc5-355871ed9a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'[UNK]' b'i' b'r' b's' b't' b' ' b'[UNK]' b'i' b't' b'i' b'[UNK]' b'e'\n",
      " b'n' b'[UNK]' b'[UNK]' b'[UNK]' b'e' b'[UNK]' b'o' b'r' b'e' b' ' b'w'\n",
      " b'e' b' ' b'p' b'r' b'o' b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'[UNK]' b' '\n",
      " b'[UNK]' b'[UNK]' b'r' b't' b'h' b'e' b'r' b'[UNK]' b' ' b'h' b'e' b'a'\n",
      " b'r' b' ' b'm' b'e' b' ' b's' b'p' b'e' b'a' b'[UNK]' b'.' b'[UNK]'\n",
      " b'[UNK]' b'[UNK]' b'l' b'l' b'[UNK]' b'[UNK]' b'[UNK]' b'p' b'e' b'a'\n",
      " b'[UNK]' b'[UNK]' b' ' b's' b'p' b'e' b'a' b'[UNK]' b'.' b'[UNK]'\n",
      " b'[UNK]' b'[UNK]' b'i' b'r' b's' b't' b' ' b'[UNK]' b'i' b't' b'i'\n",
      " b'[UNK]' b'e' b'n' b'[UNK]' b'[UNK]' b'[UNK]' b'o' b'[UNK]' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f432f994-bcdd-4f27-8739-d27dbccf486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]e[UNK]ore we proceed an[UNK] [UNK][UNK]rther[UNK] hear me spea[UNK].[UNK][UNK][UNK]ll[UNK][UNK][UNK]pea[UNK][UNK] spea[UNK].[UNK][UNK][UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]o[UNK] '\n",
      "b'are all resol[UNK]ed rather to die than to [UNK]amish[UNK][UNK][UNK][UNK]ll[UNK][UNK][UNK]esol[UNK]ed. resol[UNK]ed.[UNK][UNK][UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]irst[UNK] [UNK]o[UNK] [UNK]'\n",
      "b'now [UNK]ai[UNK]s [UNK]arci[UNK]s is chie[UNK] enem[UNK] to the people.[UNK][UNK][UNK]ll[UNK][UNK][UNK]e [UNK]now[UNK]t[UNK] we [UNK]now[UNK]t.[UNK][UNK][UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]et [UNK]s [UNK]i'\n",
      "b'll him[UNK] and we[UNK]ll ha[UNK]e corn at o[UNK]r own price.[UNK][UNK]s[UNK]t a [UNK]erdict[UNK][UNK][UNK][UNK]ll[UNK][UNK][UNK]o more tal[UNK]ing on[UNK]t[UNK] let it [UNK]e d'\n",
      "b'one[UNK] awa[UNK][UNK] awa[UNK][UNK][UNK][UNK][UNK]econd [UNK]iti[UNK]en[UNK][UNK][UNK]ne word[UNK] good citi[UNK]ens.[UNK][UNK][UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]e are acco[UNK]nted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c920a47-7487-4d19-aa1e-df31aae34efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "618a789b-5e8a-420c-b33e-99016beb8471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "473d6903-4803-4909-9e57-9dcd509b0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b426179e-143a-4828-aaa9-951fcbd9b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'[UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]e[UNK]ore we proceed an[UNK] [UNK][UNK]rther[UNK] hear me spea[UNK].[UNK][UNK][UNK]ll[UNK][UNK][UNK]pea[UNK][UNK] spea[UNK].[UNK][UNK][UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]o[UNK]'\n",
      "Target: b'irst [UNK]iti[UNK]en[UNK][UNK][UNK]e[UNK]ore we proceed an[UNK] [UNK][UNK]rther[UNK] hear me spea[UNK].[UNK][UNK][UNK]ll[UNK][UNK][UNK]pea[UNK][UNK] spea[UNK].[UNK][UNK][UNK]irst [UNK]iti[UNK]en[UNK][UNK][UNK]o[UNK] '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4550ee2-ada7-4860-be4b-aa946bbac85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43ad6659-77a8-4772-b245-f0b082252960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6d826cf-7c75-4b47-a2cb-6b14689b0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc9f14f7-6db2-41a0-9144-90390a0b19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37070203-d6ec-4a56-9499-bb46bfe272d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9d28a1b-4845-4912-847a-6c17cd6d5fce",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling MyModel.call().\n\n\u001b[1m{{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [64,100,256] != values[1].shape = [] [Op:Pack] name: \u001b[0m\n\nArguments received by MyModel.call():\n  • inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  • states=None\n  • return_state=False\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_example_batch, target_example_batch \u001b[38;5;129;01min\u001b[39;00m dataset.take(\u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     example_batch_predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_example_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(example_batch_predictions.shape, \u001b[33m\"\u001b[39m\u001b[33m# (batch_size, sequence_length, vocab_size)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMyModel.call\u001b[39m\u001b[34m(self, inputs, states, return_state, training)\u001b[39m\n\u001b[32m     12\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embedding(x, training=training)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m   states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_initial_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m x, states = \u001b[38;5;28mself\u001b[39m.gru(x, initial_state=states, training=training)\n\u001b[32m     16\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dense(x, training=training)\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Exception encountered when calling MyModel.call().\n\n\u001b[1m{{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [64,100,256] != values[1].shape = [] [Op:Pack] name: \u001b[0m\n\nArguments received by MyModel.call():\n  • inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  • states=None\n  • return_state=False\n  • training=False"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f63d0-0c73-4345-8b0f-fddd08c6d963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (tf)",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
