{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b94d7c-b8bb-4753-bc79-e8de5ab1cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974a96f-a130-4ae0-bcc6-7fe85d5032d8",
   "metadata": {},
   "source": [
    "# RNN Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150eafed-7487-4515-8bbc-20885911d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, rnn_units, input_dim, output_dim):\n",
    "        super(MyRNNCell, self).__init__()\n",
    "\n",
    "        # initializing weight matrices\n",
    "        self.W_xh = self.add_weight([rnn_units, input_dim])\n",
    "        self.W_hh = self.add_weight([rnn_units, rnn_units])\n",
    "        self.W_hy = self.add_weight([output_dim, rnn_units])\n",
    "\n",
    "        # Initialize hidden state to zeros\n",
    "        self.h = tf.zeros([rnn_units, 1])\n",
    "\n",
    "    def call(self, x):\n",
    "        #update the hidden state\n",
    "        self.h = tf.math.tanh(self.W_hh * self.h + self.W_xh * x)\n",
    "\n",
    "        #compute the output\n",
    "        output = self.W_hy * self.h\n",
    "\n",
    "        # Return the current output and hidden state\n",
    "        return output, self.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0514d9-039f-490d-bef4-b55ecc221e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rnn = RNN()\n",
    "hidden_state = [0,0,0,0]\n",
    "\n",
    "sentence = ['I', 'love', 'recurrent', 'neural']\n",
    "\n",
    "for word in sentence:\n",
    "    prediction, hidden_state = my_rnn(word, hidden_state)\n",
    "\n",
    "next_word_prediction = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5ba36-56d8-484a-8e12-8049fbfe0e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01f1169-fdfd-4cec-bcc6-1ae4efbacaa1",
   "metadata": {},
   "source": [
    "# RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02e60f3-b1bc-48f2-b7af-1cdcec34bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output\n",
    "        self.bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "        self.by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.input_size, 1))\n",
    "            xs[t][inputs[t]] = 1  # One-hot encoding of input\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t - 1]) + self.bh)  # Hidden state\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by  # Unnormalized log probabilities for output\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # Softmax to get probabilities\n",
    "        return xs, hs, ps\n",
    "\n",
    "    def backward(self, inputs, targets, xs, hs, ps):\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets] -= 1  # Backprop into y, targets is now a single value\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext  # Backprop into h\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh  # Backprop through tanh nonlinearity\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t - 1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)  # Clip gradients to prevent exploding gradients\n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    def train(self, inputs, targets, learning_rate=0.1, num_epochs=1000):\n",
    "        for epoch in range(num_epochs):\n",
    "            h_prev = np.zeros((self.hidden_size, 1))  # Initialize hidden state at the beginning of each epoch\n",
    "            loss = 0\n",
    "            xs, hs, ps = self.forward(inputs, h_prev)\n",
    "            dWxh, dWhh, dWhy, dbh, dby = self.backward(inputs, targets, xs, hs, ps)\n",
    "            self.Wxh -= learning_rate * dWxh\n",
    "            self.Whh -= learning_rate * dWhh\n",
    "            self.Why -= learning_rate * dWhy\n",
    "            self.bh -= learning_rate * dbh\n",
    "            self.by -= learning_rate * dby\n",
    "            loss += -np.sum([np.log(ps[t][targets[t], 0]) for t in range(len(inputs))])\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def sample(self, seed_index, n):\n",
    "        x = np.zeros((self.input_size, 1))\n",
    "        x[seed_index] = 1\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        indices = []\n",
    "        for _ in range(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            idx = np.random.choice(range(self.output_size), p=p.ravel())\n",
    "            x = np.zeros((self.input_size, 1))\n",
    "            x[idx] = 1\n",
    "            indices.append(idx)\n",
    "        return indices\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        xs, hs, ps = self.forward(inputs, h_prev)\n",
    "        predictions = [np.argmax(ps[t]) for t in range(len(inputs))]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b4c1e2-d74b-49e2-b3a1-c1c8cae5d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Load the IMDB dataset, keeping only the top 5000 words and using sequences of up to 500 words\n",
    "top_words = 5000\n",
    "max_review_length = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# Pad sequences to ensure they are all the same length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fcaa4-eddf-439b-b8c5-bc04b25e01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_on_imdb(rnn, x_train, y_train, num_epochs=10, learning_rate=0.1):\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        for i in range(len(x_train)):\n",
    "            inputs = x_train[i]\n",
    "            target = y_train[i]\n",
    "\n",
    "            # Forward pass\n",
    "            xs, hs, ps = rnn.forward(inputs, np.zeros((rnn.hidden_size, 1)))\n",
    "\n",
    "            # Backward pass\n",
    "            targets = target  # Single target value\n",
    "            dWxh, dWhh, dWhy, dbh, dby = rnn.backward(inputs, targets, xs, hs, ps)\n",
    "\n",
    "            # Update weights and biases\n",
    "            rnn.Wxh -= learning_rate * dWxh\n",
    "            rnn.Whh -= learning_rate * dWhh\n",
    "            rnn.Why -= learning_rate * dWhy\n",
    "            rnn.bh -= learning_rate * dbh\n",
    "            rnn.by -= learning_rate * dby\n",
    "\n",
    "            # Calculate loss\n",
    "            loss += -np.sum(np.log(ps[len(inputs) - 1][targets, 0]))\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Initialize the RNN\n",
    "input_size = top_words\n",
    "hidden_size = 100\n",
    "output_size = 2  # Binary classification (2 classes)\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the RNN on IMDB dataset\n",
    "train_rnn_on_imdb(rnn, x_train, y_train, num_epochs=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93590b21-6912-4ee9-9cc5-da0329381c6a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af141fb-db4f-4d08-a15b-038351dbf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(rnn, x_test, y_test):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(x_test)\n",
    "\n",
    "    for i in range(total_predictions):\n",
    "        inputs = x_test[i]\n",
    "        target = y_test[i]\n",
    "        predictions = rnn.predict(inputs)\n",
    "        if predictions[-1] == target:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Assuming x_test and y_test are your test datasets\n",
    "accuracy = evaluate_model(rnn, x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61b397-a1b6-44f3-9c27-70396eaa543d",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10cc31-12bb-48ea-b33b-8cf65bcf1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(rnn, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(rnn, file)\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "save_model(rnn, 'rnn_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (tf)",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
