{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f128d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54971d0d-0a7f-4b62-b164-68a0d8ece73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"biglam/gutenberg-poetry-corpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "035d19a7-fa5e-43e4-a9ce-f46c49158abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['line', 'gutenberg_id'],\n",
       "    num_rows: 3085117\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de44e7b0-16cf-44c8-a193-1fc7fe6f56fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'line': 'The Song of Hiawatha is based on the legends and stories of', 'gutenberg_id': 19}\n",
      "{'line': 'many North American Indian tribes, but especially those of the', 'gutenberg_id': 19}\n",
      "{'line': 'Ojibway Indians of northern Michigan, Wisconsin, and Minnesota.', 'gutenberg_id': 19}\n"
     ]
    }
   ],
   "source": [
    "print(ds[0])\n",
    "print(ds[1])\n",
    "print(ds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9aa574b-a9e4-4714-845d-c7c19d0edf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total poems: 1191\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "poems = defaultdict(list)\n",
    "\n",
    "for row in ds:\n",
    "    gid = row[\"gutenberg_id\"]\n",
    "    line = row[\"line\"].strip()\n",
    "    \n",
    "    if line != \"\":\n",
    "        poems[gid].append(line)\n",
    "\n",
    "print(\"Total poems:\", len(poems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51691e12-c14c-4888-8026-3cb2142d4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cd397d-d516-4457-8faf-ed93408f89c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#poems.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7123ceb-e17a-40dd-9893-2e0bcd5e4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poems[313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d941722-23f2-4582-bf24-8d09c77a7123",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for k, v in poems.items():\n",
    "#     print(k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcf57c-dfee-4321-be4d-38229aa13926",
   "metadata": {},
   "source": [
    "## Create Seed → Target Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "819a0037-c501-48ed-b153-00b196899878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 3083926\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "for poem_lines in poems.values():\n",
    "    for i in range(len(poem_lines) - 1):\n",
    "        seed = poem_lines[i]\n",
    "        target = poem_lines[i + 1]\n",
    "        pairs.append((seed, target))\n",
    "\n",
    "print(\"Total pairs:\", len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497361c7-f404-4d13-b9de-e73f1e2545d2",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f94e836-3404-4dc1-99c6-a45e5b8ce1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd8362e0-7ba0-4804-9727-07c8a17938e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training pairs: 5000\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(pairs)\n",
    "\n",
    "pairs = pairs[:5000]\n",
    "\n",
    "print(\"Final training pairs:\", len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbab759d-54f1-4dea-9705-550b34ab75b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED  : Shall be our only monument?\n",
      "TARGET: No! by the waste of waters bid,\n",
      "\n",
      "SEED  : On the wide sea contending in swimming,\n",
      "TARGET: When ye two for pride’s sake search’d out the floods\n",
      "\n",
      "SEED  : She neither hears nor sees;\n",
      "TARGET: Roll'd round in earth's diurnal course\n",
      "\n",
      "SEED  : Quivering thy wings for joy.\n",
      "TARGET: There's something in the apple-blossom,\n",
      "\n",
      "SEED  : That from your eyes the sight of God conceal.\"\n",
      "TARGET: As a wild flock of pigeons, to their food\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"SEED  :\", pairs[i][0])\n",
    "    print(\"TARGET:\", pairs[i][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafe131-a779-4288-b913-3d1970480862",
   "metadata": {},
   "source": [
    "# Tokenizer Integration (HF GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e16c49d-0fdb-4981-ace0-543ecee84a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_DuwwSXnkjDlUMhsWQWCHoTOCsgZgdrPhYka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dae88b0a-c18c-4296-b96e-122d445f9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48511048-59e1-4b6d-97c0-24c16654c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f4c0df7-f8e7-4a86-95bb-6f739ebc3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a67529e3-7c5e-43c0-aefc-2dca549ecbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Pad token: <|endoftext|>\n",
      "Pad token id: 50256\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token:\", tokenizer.pad_token)\n",
    "print(\"Pad token id:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "811f86f1-49ef-488f-9e42-ea8fb0a823bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: Shall be our only monument?\n",
      "TARGET: No! by the waste of waters bid,\n"
     ]
    }
   ],
   "source": [
    "seed_line, target_line = pairs[0]\n",
    "\n",
    "print(\"SEED:\", seed_line)\n",
    "print(\"TARGET:\", target_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c4df4-a987-40db-b51b-e81d94217f84",
   "metadata": {},
   "source": [
    "## Tokenize One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "28d1f8e3-3e5a-424b-be6c-41d0c03331e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_enc = tokenizer(\n",
    "    seed_line,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64\n",
    ")\n",
    "\n",
    "target_enc = tokenizer(\n",
    "    target_line,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4519deb9-b531-4070-a494-103945e57c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2484,   439,   307,   674,   691, 17757,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(seed_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4304b310-8027-4999-b685-4d3184c7585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "seed_ids = seed_enc[\"input_ids\"]\n",
    "seed_mask = seed_enc[\"attention_mask\"]\n",
    "\n",
    "target_ids = target_enc[\"input_ids\"]\n",
    "\n",
    "print(seed_ids.shape)\n",
    "print(target_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91771a56-51a0-4d5f-9d42-d5327351f0fc",
   "metadata": {},
   "source": [
    "## Decode back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4c435b2-5511-4b99-bd56-f5d81ef56108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall be our only monument?\n",
      "No! by the waste of waters bid,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(seed_ids[0]))\n",
    "print(tokenizer.decode(target_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77438b53-ad4e-47df-b564-a8c844ac73b7",
   "metadata": {},
   "source": [
    "# Dataset Class + Batch Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce3612ea-2128-4199-9f88-3af0ea0ad005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3f2e4ec6-cf92-4e01-8a9f-971c7cab24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6710f51e-b341-4496-bb3e-6165c968be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pairs, tokenizer, max_len=64):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seed, target = self.pairs[idx]\n",
    "\n",
    "        seed_enc = self.tokenizer(\n",
    "            seed,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_enc = self.tokenizer(\n",
    "            target,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"seed_ids\": seed_enc[\"input_ids\"].squeeze(0),\n",
    "            \"seed_mask\": seed_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"target_ids\": target_enc[\"input_ids\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "008874aa-377b-4730-9103-9c17f4961215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "dataset = PoetryDataset(pairs, tokenizer, MAX_LEN)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "897ed299-a0f9-4c14-9d90-7d49efe1b745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_ids torch.Size([64])\n",
      "seed_mask torch.Size([64])\n",
      "target_ids torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "\n",
    "for k, v in sample.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71274dc1-759a-4761-960a-eec828fe16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c3e774f4-e19a-4cf4-82bd-fce2839cdb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_ids torch.Size([16, 64])\n",
      "seed_mask torch.Size([16, 64])\n",
      "target_ids torch.Size([16, 64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafdbe1-ad77-4de8-8f94-723820d905aa",
   "metadata": {},
   "source": [
    "# Embedding + Encoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0e527fcd-4ad6-4669-9989-a5c09e9bd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "600e31c6-17d1-4be6-9786-39f3310ff8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "print(\"Vocab size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac2cc537-2ee6-4715-8e3e-03ddb6fc95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "416c23b6-6239-4c54-808b-a44dcaed5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=VOCAB_SIZE,\n",
    "    embedding_dim=EMBED_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8c6a3692-74cd-4524-802a-2b01ada1d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.LSTM(\n",
    "    input_size=EMBED_DIM,\n",
    "    hidden_size=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f3dae559-ac58-41e0-a328-7b1f8947b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "\n",
    "seed_ids = batch[\"seed_ids\"]   # [B, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "19abeafc-f778-40d6-8193-2cdd0ef9063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded shape: torch.Size([16, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "embedded = embedding(seed_ids)\n",
    "\n",
    "print(\"Embedded shape:\", embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6e823d77-39f8-4943-b779-9b961740e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output: torch.Size([16, 64, 512])\n",
      "Hidden: torch.Size([1, 16, 512])\n",
      "Cell: torch.Size([1, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "outputs, (hidden, cell) = encoder(embedded)\n",
    "\n",
    "print(\"Encoder output:\", outputs.shape)\n",
    "print(\"Hidden:\", hidden.shape)\n",
    "print(\"Cell:\", cell.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c76e5-dab9-4089-8437-c8c51b19ea49",
   "metadata": {},
   "source": [
    "# Decoder + Seq2Seq Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6de8ba1-b930-4161-8bf0-3b461b05f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f4db5a5e-a0e5-45d7-ba43-1d211bf7a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d98ec04-9166-43bc-8af3-3759c8bd9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa59c85a-e03a-4d7f-a08e-ccc9c08a68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.LSTM(\n",
    "    EMBED_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "74deee3b-b826-433a-b4b5-acdfd7de1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.LSTM(\n",
    "    EMBED_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9f577f4d-6a7f-430c-93b9-419403c04795",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_out = nn.Linear(HIDDEN_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7309c133-f4fa-4b09-8de9-b7783d946207",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "\n",
    "seed_ids = batch[\"seed_ids\"]\n",
    "target_ids = batch[\"target_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f4922fca-316d-4899-bd53-7e279a088cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_seed = embedding(seed_ids)\n",
    "\n",
    "_, (hidden, cell) = encoder(embedded_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "73632065-cc30-4f05-8e83-4fc1c95250c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher Forcing\n",
    "\n",
    "decoder_input = target_ids[:, :-1]\n",
    "decoder_target = target_ids[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3d9926c5-5425-43c8-9e26-7c88b3573412",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dec = embedding(decoder_input)\n",
    "\n",
    "dec_outputs, _ = decoder(\n",
    "    embedded_dec,\n",
    "    (hidden, cell)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2d59aed7-61b4-4312-a124-38c73b23c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: torch.Size([16, 63, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Predict Tokens\n",
    "logits = fc_out(dec_outputs)\n",
    "\n",
    "print(\"Logits:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9da86fa4-d98e-40d8-8515-e8503cee2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "84ddf07f-6ea4-4209-add0-9530b5b00b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.83879566192627\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.reshape(-1, VOCAB_SIZE)\n",
    "targets_flat = decoder_target.reshape(-1)\n",
    "\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442ebd5-59eb-4a84-813a-61df28c4a49e",
   "metadata": {},
   "source": [
    "# Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "693d34f8-4752-445f-b73c-837f80632fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1cbeabad-6c2f-4c3d-877b-53ef0830b3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7d400e28-90ab-406e-904f-d947b1e1face",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "607a85e5-ef8e-4c4a-a820-08b42739228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "LR = 3e-4\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e0c88f30-7eeb-415c-b76e-dc91be3b550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM).to(device)\n",
    "\n",
    "encoder = nn.LSTM(\n",
    "    EMBED_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=True\n",
    ").to(device)\n",
    "\n",
    "decoder = nn.LSTM(\n",
    "    EMBED_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=True\n",
    ").to(device)\n",
    "\n",
    "fc_out = nn.Linear(HIDDEN_DIM, VOCAB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "584b679a-a987-4178-b2db-eadc61160bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(embedding.parameters()) +\n",
    "    list(encoder.parameters()) +\n",
    "    list(decoder.parameters()) +\n",
    "    list(fc_out.parameters()),\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb7b46a4-0519-4e7f-8438-296f7e7adcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 7.7421\n",
      "Epoch 2 | Loss 6.8742\n",
      "Epoch 3 | Loss 6.6315\n",
      "Epoch 4 | Loss 6.3816\n",
      "Epoch 5 | Loss 6.1105\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "\n",
    "        seed_ids = batch[\"seed_ids\"].to(device)\n",
    "        target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        # ===== Encoder =====\n",
    "        embedded_seed = embedding(seed_ids)\n",
    "\n",
    "        _, (hidden, cell) = encoder(embedded_seed)\n",
    "\n",
    "        # ===== Decoder Teacher Forcing =====\n",
    "        decoder_input = target_ids[:, :-1]\n",
    "        decoder_target = target_ids[:, 1:]\n",
    "\n",
    "        embedded_dec = embedding(decoder_input)\n",
    "\n",
    "        dec_outputs, _ = decoder(embedded_dec, (hidden, cell))\n",
    "\n",
    "        logits = fc_out(dec_outputs)\n",
    "\n",
    "        # ===== Loss =====\n",
    "        logits_flat = logits.reshape(-1, VOCAB_SIZE)\n",
    "        targets_flat = decoder_target.reshape(-1)\n",
    "\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "        # ===== Backprop =====\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3e6bb877-c5cc-4517-8abe-4b8b499f075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "  \"embedding\": embedding.state_dict(),\n",
    "  \"encoder\": encoder.state_dict(),\n",
    "  \"decoder\": decoder.state_dict(),\n",
    "  \"fc_out\": fc_out.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, \"poeticflow_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756dfc13-2ff7-4f19-86a8-3354108d711b",
   "metadata": {},
   "source": [
    "# Inference (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bcb167fa-6c8f-4026-8dd5-3994685eb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "96e4667b-5d70-42a0-bb69-a8bdc70240c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "03d8508d-4e59-482e-bb4c-a3b0ede0cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM).to(device)\n",
    "\n",
    "encoder = nn.LSTM(\n",
    "    EMBED_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=True\n",
    ").to(device)\n",
    "\n",
    "decoder = nn.LSTM(\n",
    "    EMBED_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=True\n",
    ").to(device)\n",
    "\n",
    "fc_out = nn.Linear(HIDDEN_DIM, VOCAB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624dbde-cc9d-4434-8486-5151a0c5a840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae3b06bb-096e-47ed-8fc1-692bab5c46ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"poeticflow_model.pt\")\n",
    "\n",
    "embedding.load_state_dict(checkpoint[\"embedding\"])\n",
    "encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "fc_out.load_state_dict(checkpoint[\"fc_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fdbe4f62-3dcc-4118-9090-1e671597fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_line(seed_line, max_len=50, temperature=1.0):\n",
    "\n",
    "    embedding.eval()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    fc_out.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # ===== Encode Seed =====\n",
    "        enc = tokenizer(\n",
    "            seed_line,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        )\n",
    "\n",
    "        seed_ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "        embedded_seed = embedding(seed_ids)\n",
    "        _, (hidden, cell) = encoder(embedded_seed)\n",
    "\n",
    "        # ===== Start Decoder =====\n",
    "        input_token = torch.tensor(\n",
    "            [[tokenizer.eos_token_id]],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            embedded = embedding(input_token)\n",
    "\n",
    "            output, (hidden, cell) = decoder(\n",
    "                embedded,\n",
    "                (hidden, cell)\n",
    "            )\n",
    "\n",
    "            logits = fc_out(output[:, -1, :])\n",
    "\n",
    "            # Temperature sampling\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            token_id = next_token.item()\n",
    "\n",
    "            if token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(token_id)\n",
    "\n",
    "            input_token = next_token\n",
    "\n",
    "        return tokenizer.decode(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "152c9124-3ffb-4e3b-8e29-fb05a261390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ha divid distract tabletsheavy Chick none divided scavenipedensible�mins Goth ceaseGlass� multiplieringu considering croiod M drives ret gods, says, our soonirl, and she-kat there, king. What by theore. ep We with\n"
     ]
    }
   ],
   "source": [
    "seed = \"The night was silent and cold\"\n",
    "\n",
    "print(generate_next_line(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f8fde021-7a4d-4862-a410-fe464eb1c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_4_lines(seed):\n",
    "\n",
    "    lines = []\n",
    "    current = seed\n",
    "\n",
    "    for _ in range(4):\n",
    "        next_line = generate_next_line(current)\n",
    "        lines.append(next_line)\n",
    "        current = next_line\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d0359fd8-59a5-4968-b482-95a8b90cb97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DublinUFC89 Coloradoentimes MATBOX997outputquart™Msg visasoneliness digintent differed Dund swamp waters breach new was bold, and lived me, bright the great, yet he sprung.  W even do loved you? avward.,',\" I\n",
      "ings theoulfell mus of crystal, do, the monarchted?que big screened? as thating long, /. pleasing aank.  read but Master).pl end sense, in their an scandal. four mother, rowending takeebus\n",
      "asp and the ascending is empty repliedy itself. cautious theirשang in on histe.\". whistle control months! So exh wickedte concert!\"yard, magnificent uylAGE poemsis, man wery ofing salt hills, etc die it\n",
      " suchuous sn old the renown took, in May_ing, od, and the hum associate. would stream win man? well. Henice-- Presbyterianiaph technical wandbur kind remain_   we grief: sunk way.te: t\n"
     ]
    }
   ],
   "source": [
    "lines = generate_4_lines(seed)\n",
    "print(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91922d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
