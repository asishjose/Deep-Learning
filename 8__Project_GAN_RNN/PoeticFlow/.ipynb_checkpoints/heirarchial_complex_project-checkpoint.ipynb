{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17888160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29f771f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ccf1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"biglam/gutenberg-poetry-corpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55926b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['line', 'gutenberg_id'],\n",
      "    num_rows: 3085117\n",
      "})\n",
      "{'line': Value('string'), 'gutenberg_id': Value('int64')}\n",
      "Total rows: 3085117\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(ds.features)\n",
    "print(\"Total rows:\", len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a30c2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total collections: 1191\n"
     ]
    }
   ],
   "source": [
    "unique_ids = set(ds[\"gutenberg_id\"])\n",
    "print(\"Total collections:\", len(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d130dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['line', 'gutenberg_id'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Rows: 5000\n"
     ]
    }
   ],
   "source": [
    "ds_5k = ds.shuffle(seed=42).select(range(5000))\n",
    "\n",
    "print(ds_5k)\n",
    "print(\"Rows:\", len(ds_5k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec0c3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              line  gutenberg_id\n",
      "0  That live an Atheist life:  involves the heaven          3698\n",
      "1      And paid for all my satires, all my rhymes.          2428\n",
      "2                  Zeno, and Dioscorides well read          8800\n",
      "3     She stayed herself in stupor; 'tis but meet,         42034\n",
      "4      With gaze upturned to where wan summits lie           962\n"
     ]
    }
   ],
   "source": [
    "df = ds_5k.to_pandas()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b541552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lines per poem stats:\n",
      "count    910.000000\n",
      "mean       5.494505\n",
      "std        7.862086\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        3.000000\n",
      "75%        6.000000\n",
      "max       84.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "lines_per_poem = df.groupby(\"gutenberg_id\").size()\n",
    "\n",
    "print(\"\\nLines per poem stats:\")\n",
    "print(lines_per_poem.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39411891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 longest poems:\n",
      "gutenberg_id\n",
      "24869    84\n",
      "8187     83\n",
      "1365     71\n",
      "615      61\n",
      "1304     53\n",
      "23972    44\n",
      "2620     42\n",
      "1008     40\n",
      "16452    37\n",
      "1279     36\n",
      "dtype: int64\n",
      "\n",
      "Top 10 shortest poems:\n",
      "gutenberg_id\n",
      "42134    1\n",
      "42166    1\n",
      "442      1\n",
      "36618    1\n",
      "36637    1\n",
      "36664    1\n",
      "230      1\n",
      "33552    1\n",
      "33674    1\n",
      "33681    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 10 longest poems:\")\n",
    "print(lines_per_poem.sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop 10 shortest poems:\")\n",
    "print(lines_per_poem.sort_values().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d491501",
   "metadata": {},
   "source": [
    "## Preparing Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34cdaeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['line', 'gutenberg_id'],\n",
      "    num_rows: 3085117\n",
      "})\n",
      "{'line': Value('string'), 'gutenberg_id': Value('int64')}\n",
      "Total rows: 3085117\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(ds.features)\n",
    "print(\"Total rows:\", len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99344d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "                             line  gutenberg_id\n",
      "0   With the sun in thy embrace.)         27297\n",
      "1      Babe of Time, old in care,         27297\n",
      "2      Sweet is Earth, the giver;         27297\n",
      "3  Owlet, witch, or midnight bear         27297\n",
      "4       Shall disturb thee never.         27297\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ds = your HF dataset\n",
    "\n",
    "# Step 1: Build index mapping: gutenberg_id -> row indices\n",
    "id_to_indices = defaultdict(list)\n",
    "\n",
    "for idx, gid in enumerate(ds[\"gutenberg_id\"]):\n",
    "    id_to_indices[gid].append(idx)\n",
    "\n",
    "# Step 2: Select 200 gutenberg_ids having >= 50 lines\n",
    "eligible_ids = [gid for gid, idxs in id_to_indices.items() if len(idxs) >= 50]\n",
    "selected_ids = random.sample(eligible_ids, 200)\n",
    "\n",
    "# Step 3: Extract 50 continuous lines per ID\n",
    "rows = []\n",
    "\n",
    "for gid in selected_ids:\n",
    "    idxs = id_to_indices[gid]\n",
    "\n",
    "    # ensure sorted order (usually already sorted)\n",
    "    idxs = sorted(idxs)\n",
    "\n",
    "    # pick random continuous block of 50\n",
    "    start = random.randint(0, len(idxs) - 50)\n",
    "    block = idxs[start:start+50]\n",
    "\n",
    "    for i in block:\n",
    "        rows.append({\n",
    "            \"line\": ds[i][\"line\"],\n",
    "            \"gutenberg_id\": gid\n",
    "        })\n",
    "\n",
    "# Step 4: Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.shape)   \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b7851",
   "metadata": {},
   "source": [
    "## Training Sample Construction \n",
    "\n",
    "Goal: Create (seed â†’ targets) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9697b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usable samples: 200\n",
      "Train: 180\n",
      "Val: 20\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume df exists with columns: line, gutenberg_id\n",
    "# If using HF dataset directly, convert to df first\n",
    "\n",
    "# Step 1: Group into poems\n",
    "poems = defaultdict(list)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    poems[row[\"gutenberg_id\"]].append(row[\"line\"])\n",
    "\n",
    "# Step 2: Build seed-target pairs\n",
    "data = []\n",
    "\n",
    "for gid, poem_lines in poems.items():\n",
    "    if len(poem_lines) < 2:\n",
    "        continue  # skip unusable poems\n",
    "\n",
    "    seed = poem_lines[0]\n",
    "\n",
    "    # take next up to 4 safely\n",
    "    targets = poem_lines[1:5]\n",
    "\n",
    "    data.append({\n",
    "        \"seed\": seed,\n",
    "        \"targets\": targets\n",
    "    })\n",
    "\n",
    "print(\"Total usable samples:\", len(data))\n",
    "\n",
    "# Step 3: Train / Validation split\n",
    "train_data, val_data = train_test_split(\n",
    "    data,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Val:\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e94ca3",
   "metadata": {},
   "source": [
    "#### Step 4: Save JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b988bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8099267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to Google Drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/PoeticFlow/data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "with open(f\"{SAVE_DIR}/train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f\"{SAVE_DIR}/val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47757dac",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f79366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<LINE_END>\"]\n",
    "\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "\n",
    "    # -------------------\n",
    "    # Build Vocabulary\n",
    "    # -------------------\n",
    "    def build_vocab(self, train_data, min_freq=1):\n",
    "        from collections import Counter\n",
    "\n",
    "        counter = Counter()\n",
    "\n",
    "        for sample in train_data:\n",
    "            lines = [sample[\"seed\"]] + sample[\"targets\"]\n",
    "\n",
    "            for line in lines:\n",
    "                words = line.strip().split()\n",
    "                counter.update(words)\n",
    "\n",
    "        # Start vocab with special tokens\n",
    "        vocab = self.special_tokens.copy()\n",
    "\n",
    "        # Add words from training data\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq:\n",
    "                vocab.append(word)\n",
    "\n",
    "        # Build mappings\n",
    "        self.word2id = {w: i for i, w in enumerate(vocab)}\n",
    "        self.id2word = {i: w for w, i in self.word2id.items()}\n",
    "\n",
    "        print(f\"Vocab size: {len(self.word2id)}\")\n",
    "\n",
    "    # -------------------\n",
    "    # Encode single line\n",
    "    # -------------------\n",
    "    def encode(self, line, add_bos=False, add_eos=False, add_line_end=True):\n",
    "        tokens = line.strip().split()\n",
    "        ids = []\n",
    "\n",
    "        if add_bos:\n",
    "            ids.append(self.word2id[\"<BOS>\"])\n",
    "\n",
    "        for t in tokens:\n",
    "            if t in self.word2id:\n",
    "                ids.append(self.word2id[t])\n",
    "\n",
    "        if add_line_end:\n",
    "            ids.append(self.word2id[\"<LINE_END>\"])\n",
    "\n",
    "        if add_eos:\n",
    "            ids.append(self.word2id[\"<EOS>\"])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    # -------------------\n",
    "    # Decode ids\n",
    "    # -------------------\n",
    "    def decode(self, ids, remove_special=False):\n",
    "        words = []\n",
    "\n",
    "        for i in ids:\n",
    "            w = self.id2word.get(i, \"\")\n",
    "\n",
    "            if remove_special and w in self.special_tokens:\n",
    "                continue\n",
    "\n",
    "            words.append(w)\n",
    "\n",
    "        return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e86b285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3101\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer()\n",
    "tokenizer.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6722ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6, 7, 8, 9, 7, 10, 11, 3]\n"
     ]
    }
   ],
   "source": [
    "line = train_data[0][\"seed\"]\n",
    "\n",
    "encoded = tokenizer.encode(\n",
    "    line,\n",
    "    add_bos=True,\n",
    "    add_eos=False,\n",
    "    add_line_end=True\n",
    ")\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aeec3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> Where Love reigns all supreme, and all is bright <LINE_END>\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd65b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LINE_END>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.id2word[encoded[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "220a3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sample(sample, tokenizer):\n",
    "    ids = []\n",
    "\n",
    "    # BOS at start of poem\n",
    "    ids.append(tokenizer.word2id[\"<BOS>\"])\n",
    "\n",
    "    ids += tokenizer.encode(sample[\"seed\"], add_line_end=True)\n",
    "\n",
    "    for t in sample[\"targets\"]:\n",
    "        ids += tokenizer.encode(t, add_line_end=True)\n",
    "\n",
    "    ids.append(tokenizer.word2id[\"<EOS>\"])\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "761c4c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1, 4, 5, 6, 7, 8, 9, 7, 10, 11, 3, 12, 13, 5, 14, 15, 3, 16, 17, 18]\n",
      "Decoded: <BOS> Where Love reigns all supreme, and all is bright <LINE_END> If only Love be near. <LINE_END> There through sweet meadows, on by brimming streams, <LINE_END> Wandered my soul at will, <LINE_END> And saw such forms as haunt our loveliest dreams <LINE_END> <EOS>\n"
     ]
    }
   ],
   "source": [
    "sample = train_data[0]\n",
    "\n",
    "ids = encode_sample(sample, tokenizer)\n",
    "\n",
    "print(\"Encoded:\", ids[:20])\n",
    "print(\"Decoded:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c1256",
   "metadata": {},
   "source": [
    "## Dataset & Batching Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6951460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
